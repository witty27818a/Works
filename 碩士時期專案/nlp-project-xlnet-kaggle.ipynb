{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.metrics import f1_score\nimport torch","metadata":{"id":"2-RsRG2tuea8","execution":{"iopub.status.busy":"2022-06-05T12:09:03.807431Z","iopub.execute_input":"2022-06-05T12:09:03.808163Z","iopub.status.idle":"2022-06-05T12:09:06.298701Z","shell.execute_reply.started":"2022-06-05T12:09:03.808082Z","shell.execute_reply":"2022-06-05T12:09:06.29773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom tqdm import tqdm","metadata":{"id":"k9jOLc43uebJ","execution":{"iopub.status.busy":"2022-06-05T12:09:06.304541Z","iopub.execute_input":"2022-06-05T12:09:06.30773Z","iopub.status.idle":"2022-06-05T12:09:06.314261Z","shell.execute_reply.started":"2022-06-05T12:09:06.307685Z","shell.execute_reply":"2022-06-05T12:09:06.312737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 設定超參數","metadata":{}},{"cell_type":"code","source":"# set random seeds for all, for reproducibility\ndef random_seed(seed = 1337):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)","metadata":{"id":"7VDXZExZuebJ","execution":{"iopub.status.busy":"2022-06-05T12:09:06.320732Z","iopub.execute_input":"2022-06-05T12:09:06.323225Z","iopub.status.idle":"2022-06-05T12:09:06.331091Z","shell.execute_reply.started":"2022-06-05T12:09:06.32318Z","shell.execute_reply":"2022-06-05T12:09:06.330097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = 400\nbatch_size = 20\nepochs = 4\nwarmup_epochs = 0\nsave_model = 1\nlr = 3e-5\nseed = 1337","metadata":{"id":"jz1z6AtduebI","execution":{"iopub.status.busy":"2022-06-05T12:09:06.337593Z","iopub.execute_input":"2022-06-05T12:09:06.3393Z","iopub.status.idle":"2022-06-05T12:09:06.348394Z","shell.execute_reply.started":"2022-06-05T12:09:06.339244Z","shell.execute_reply":"2022-06-05T12:09:06.347237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:06.352401Z","iopub.execute_input":"2022-06-05T12:09:06.353624Z","iopub.status.idle":"2022-06-05T12:09:06.366667Z","shell.execute_reply.started":"2022-06-05T12:09:06.353586Z","shell.execute_reply":"2022-06-05T12:09:06.364116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 匯入資料","metadata":{}},{"cell_type":"code","source":"# os.getcwd()\n# os.chdir(\"C:\\\\Users\\\\GL75\\\\OneDrive\\\\桌面\\\\自然語言處理\\\\期末project\\\\data\")","metadata":{"id":"nLqUgIM7uea-","outputId":"d95d69f3-a4e5-45c1-ab66-754f7cd58a96","execution":{"iopub.status.busy":"2022-06-04T13:31:00.501796Z","iopub.execute_input":"2022-06-04T13:31:00.502415Z","iopub.status.idle":"2022-06-04T13:31:00.509414Z","shell.execute_reply.started":"2022-06-04T13:31:00.502374Z","shell.execute_reply":"2022-06-04T13:31:00.50841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/nlp-class-fixed-data/fixed_train.csv')\ndf_valid = pd.read_csv('../input/nlp-class-fixed-data/fixed_valid.csv')\ndf_test = pd.read_csv(\"../input/nlp-class-fixed-data/fixed_test.csv\")","metadata":{"id":"Mcry6Vz2uea_","outputId":"0c8a2cab-dc76-498a-ad94-6152988b3867","execution":{"iopub.status.busy":"2022-06-05T12:09:18.239739Z","iopub.execute_input":"2022-06-05T12:09:18.240122Z","iopub.status.idle":"2022-06-05T12:09:18.674627Z","shell.execute_reply.started":"2022-06-05T12:09:18.240092Z","shell.execute_reply":"2022-06-05T12:09:18.673784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 資料清理與合併","metadata":{"id":"eyhSq1HFuebA"}},{"cell_type":"code","source":"import nltk\nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")\nnltk.download(\"wordnet\")\nnltk.download(\"averaged_perceptron_tagger\")","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:24.14933Z","iopub.execute_input":"2022-06-05T12:09:24.149703Z","iopub.status.idle":"2022-06-05T12:09:24.921211Z","shell.execute_reply.started":"2022-06-05T12:09:24.149654Z","shell.execute_reply":"2022-06-05T12:09:24.920286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:32.301954Z","iopub.execute_input":"2022-06-05T12:09:32.302405Z","iopub.status.idle":"2022-06-05T12:09:32.306865Z","shell.execute_reply.started":"2022-06-05T12:09:32.302374Z","shell.execute_reply":"2022-06-05T12:09:32.305978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:34.94031Z","iopub.execute_input":"2022-06-05T12:09:34.940891Z","iopub.status.idle":"2022-06-05T12:09:34.945108Z","shell.execute_reply.started":"2022-06-05T12:09:34.940847Z","shell.execute_reply":"2022-06-05T12:09:34.944053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the set of stopwords. we will delete these words from our texts\nstop = set(stopwords.words(\"english\"))","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:36.357652Z","iopub.execute_input":"2022-06-05T12:09:36.358219Z","iopub.status.idle":"2022-06-05T12:09:36.365794Z","shell.execute_reply.started":"2022-06-05T12:09:36.358185Z","shell.execute_reply":"2022-06-05T12:09:36.364663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a lemmatizer object to conduct word lemmatization.\nlemmatizer = WordNetLemmatizer()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:38.98881Z","iopub.execute_input":"2022-06-05T12:09:38.989184Z","iopub.status.idle":"2022-06-05T12:09:38.994163Z","shell.execute_reply.started":"2022-06-05T12:09:38.989154Z","shell.execute_reply":"2022-06-05T12:09:38.992635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tag_map = defaultdict(lambda: wordnet.NOUN)\ntag_map['J'] = wordnet.ADJ\ntag_map['V'] = wordnet.VERB\ntag_map['R'] = wordnet.ADV","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:40.853636Z","iopub.execute_input":"2022-06-05T12:09:40.85451Z","iopub.status.idle":"2022-06-05T12:09:42.911689Z","shell.execute_reply.started":"2022-06-05T12:09:40.854474Z","shell.execute_reply":"2022-06-05T12:09:42.910713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def nltk_process(text):\n    # tokenize the sentence\n    tokenized = word_tokenize(text)\n    lst = []\n    for token, tag in pos_tag(tokenized):\n        # lemmatize each token based on their pos tagging.\n        lst.append(lemmatizer.lemmatize(token, pos = tag_map[tag[0]]))\n    for i in lst:\n        if i.lower() in stop:\n            # if the word is a stopword, remove it\n            lst.remove(i)\n    # concatenate all word tokens left into a sentence again.\n    text_cleaned = ' '.join(lst)\n    return text_cleaned","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:44.357339Z","iopub.execute_input":"2022-06-05T12:09:44.358191Z","iopub.status.idle":"2022-06-05T12:09:44.365322Z","shell.execute_reply.started":"2022-06-05T12:09:44.358153Z","shell.execute_reply":"2022-06-05T12:09:44.364028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean(text):\n    text = text.apply(lambda r: r.replace(\"_comma_\", \"\")) # clean the \"_comma_\" sign\n    text = text.apply(lambda r: r.replace(\".\", \"\")) # clean the dots\n    text = text.apply(lambda r: r.replace(\":(\", \"\")) # clean the emogi\n    text = text.apply(lambda r: r.replace(\"?\", \"\")) # clean the question mark\n    text = text.apply(lambda r: r.replace(\"!\", \"\"))\n    text = text.apply(lambda r: r.replace('\"', ''))\n    text = text.apply(lambda r: r.strip()) # clean the trailing and leading spaces\n    text = text.apply(lambda r: r.lower()) # lower the cases\n    text = text.apply(nltk_process) # do the those preprocesses using NLTK packages.\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:46.939353Z","iopub.execute_input":"2022-06-05T12:09:46.939891Z","iopub.status.idle":"2022-06-05T12:09:46.952627Z","shell.execute_reply.started":"2022-06-05T12:09:46.93985Z","shell.execute_reply":"2022-06-05T12:09:46.951572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_alittle(text):\n    text = text.apply(lambda r: r.replace(\"_comma_\", \"\"))\n    text = text.apply(lambda r: r.replace(\"..\", \"\"))\n    text = text.apply(lambda r: r.replace(\":(\", \"\"))\n    text = text.apply(lambda r: r.replace(\"?\", \"\"))\n    text = text.apply(lambda r: r.replace(\"!\", \"\"))\n    text = text.apply(lambda r: r.strip())\n    text = text.apply(lambda r: r.lower())\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:09:48.231977Z","iopub.execute_input":"2022-06-05T12:09:48.232445Z","iopub.status.idle":"2022-06-05T12:09:48.245327Z","shell.execute_reply.started":"2022-06-05T12:09:48.232406Z","shell.execute_reply":"2022-06-05T12:09:48.244277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for utterances, we clean the punctuations, do the lemmatizations and remove stopwords.\n# but for prompts, we only clean the punctuations, for prompts are quite important information.\ndf_train['utterance_cleaned'] = clean(df_train['utterance'])\ndf_train['prompt_cleaned'] = clean_alittle(df_train['prompt'])\n\ndf_valid['utterance_cleaned'] = clean(df_valid['utterance'])\ndf_valid['prompt_cleaned'] = clean_alittle(df_valid['prompt'])\n\ndf_test['utterance_cleaned'] = clean(df_test['utterance'])\ndf_test['prompt_cleaned'] = clean_alittle(df_test['prompt'])","metadata":{"id":"bms4W_jtuebB","execution":{"iopub.status.busy":"2022-06-05T12:09:52.064576Z","iopub.execute_input":"2022-06-05T12:09:52.064969Z","iopub.status.idle":"2022-06-05T12:11:58.81708Z","shell.execute_reply.started":"2022-06-05T12:09:52.064936Z","shell.execute_reply":"2022-06-05T12:11:58.81628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Left the cleaned utterances and prompts only\ndf_train.drop(['utterance_idx', 'prompt', 'utterance'], axis = 1, inplace = True)\ndf_valid.drop(['utterance_idx', 'prompt', 'utterance'], axis = 1, inplace = True)\ndf_test.drop(['utterance_idx', 'prompt', 'utterance'], axis = 1, inplace = True)","metadata":{"id":"z05HFX57uebD","execution":{"iopub.status.busy":"2022-06-05T12:11:58.819159Z","iopub.execute_input":"2022-06-05T12:11:58.820051Z","iopub.status.idle":"2022-06-05T12:11:58.866881Z","shell.execute_reply.started":"2022-06-05T12:11:58.820013Z","shell.execute_reply":"2022-06-05T12:11:58.86606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# groupby the dataframe by \"conv_id\" only. we will join all sentences in an utterance into a really long sentence.\ndf_train_gby = df_train.groupby(by = ['conv_id'], as_index = True).agg({'label': 'first', 'utterance_cleaned': ' '.join, 'prompt_cleaned': 'first'}) # trust that utterance_idx are in order\ndf_valid_gby = df_valid.groupby(by = ['conv_id'], as_index = True).agg({'label': 'first', 'utterance_cleaned': ' '.join, 'prompt_cleaned': 'first'})\ndf_test_gby = df_test.groupby(by = ['conv_id'], as_index = True).agg({'utterance_cleaned': ' '.join, 'prompt_cleaned': 'first'})","metadata":{"id":"JRJ42hvQuebE","execution":{"iopub.status.busy":"2022-06-05T12:11:58.868046Z","iopub.execute_input":"2022-06-05T12:11:58.868886Z","iopub.status.idle":"2022-06-05T12:11:59.126651Z","shell.execute_reply.started":"2022-06-05T12:11:58.868848Z","shell.execute_reply":"2022-06-05T12:11:59.125819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# further concatenate \"prompt\" and the \"utterances\" sentence into a single sentence, split by <SEP>\ndf_train_gby['merged'] = df_train_gby.prompt_cleaned.str.cat(df_train_gby.utterance_cleaned, sep = '<SEP>')\ndf_valid_gby['merged'] = df_valid_gby.prompt_cleaned.str.cat(df_valid_gby.utterance_cleaned, sep = '<SEP>')\ndf_test_gby['merged'] = df_test_gby.prompt_cleaned.str.cat(df_test_gby.utterance_cleaned, sep = '<SEP>')","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:11:59.128888Z","iopub.execute_input":"2022-06-05T12:11:59.129311Z","iopub.status.idle":"2022-06-05T12:11:59.166104Z","shell.execute_reply.started":"2022-06-05T12:11:59.129275Z","shell.execute_reply":"2022-06-05T12:11:59.165325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check if gpu is available and move to gpu\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n    \nprint(\"Device:\", torch.cuda.get_device_name(0))","metadata":{"id":"TBsXDhtIuebE","outputId":"162a6f6f-7886-4ae6-ec38-3be8bfe015a8","execution":{"iopub.status.busy":"2022-06-05T12:11:59.167388Z","iopub.execute_input":"2022-06-05T12:11:59.167818Z","iopub.status.idle":"2022-06-05T12:11:59.224042Z","shell.execute_reply.started":"2022-06-05T12:11:59.167781Z","shell.execute_reply":"2022-06-05T12:11:59.223061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import XLNetTokenizer\n\n# Load the XLNet tokenizer in\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case = True)","metadata":{"id":"M6g8HXpRuebF","outputId":"9b49e49c-44d5-466b-a876-723c6a8a577e","execution":{"iopub.status.busy":"2022-06-05T12:11:59.225432Z","iopub.execute_input":"2022-06-05T12:11:59.226092Z","iopub.status.idle":"2022-06-05T12:12:11.414008Z","shell.execute_reply.started":"2022-06-05T12:11:59.226051Z","shell.execute_reply":"2022-06-05T12:12:11.413211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a function to tokenize a set of texts\ndef data_preprocessing(corpus, max_length = max_length):\n    # create empty lists to store outputs\n    input_ids = [] # tensor of token ids to be fed to a XLNet model. (torch.Tensor)\n    token_type_ids = [] # tensor indicating token types. 0 for the first sentence (prompt) and 1 for the second sentence (lemmatized and concatenated utterances)\n    attention_masks = [] # tensor of indices specifying which tokens should be attended by the model\n    \n    for data in corpus:\n        # split the sentences in \"data\" by \"<SEP>\". The first sentence is \"prompt\" and the second sentence is \"utterances\"\n        sents = data.split(\"<SEP>\")\n        # tokenize these 2 sentences. use padding the max length we set earlier. truncate the utterance if too long.\n        enc_dict = tokenizer(sents[0], sents[1], add_special_tokens = True, padding = 'max_length', max_length = max_length, truncation = 'only_second')\n        # put the tokenized tokens, token types and attention masks to the lists to store.\n        input_ids.append(enc_dict.get('input_ids'))\n        token_type_ids.append(enc_dict.get('token_type_ids'))\n        attention_masks.append(enc_dict.get('attention_mask'))\n    # convert lists to torch tensors\n    input_ids = torch.tensor(input_ids)\n    token_type_ids = torch.tensor(token_type_ids)\n    attention_masks = torch.tensor(attention_masks)\n    \n    return input_ids, token_type_ids, attention_masks","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:12:11.415215Z","iopub.execute_input":"2022-06-05T12:12:11.415664Z","iopub.status.idle":"2022-06-05T12:12:11.42605Z","shell.execute_reply.started":"2022-06-05T12:12:11.415626Z","shell.execute_reply":"2022-06-05T12:12:11.425199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input the 'merged' column (each data is a long sentence containing the prompt and utterance, separated by <SEP>)\n# and get the tokens tensor, token types tensor and attention masks tensor\ntrain_tokens, train_types, train_masks = data_preprocessing(df_train_gby['merged'])\nvalid_tokens, valid_types, valid_masks = data_preprocessing(df_valid_gby['merged'])\ntest_tokens, test_types, test_masks = data_preprocessing(df_test_gby['merged'])","metadata":{"id":"6zEht1EruebG","outputId":"9a09517d-3951-45e6-c7e4-db7f269b0494","execution":{"iopub.status.busy":"2022-06-05T12:12:11.427649Z","iopub.execute_input":"2022-06-05T12:12:11.428256Z","iopub.status.idle":"2022-06-05T12:12:40.606716Z","shell.execute_reply.started":"2022-06-05T12:12:11.428218Z","shell.execute_reply":"2022-06-05T12:12:40.605572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transform labels to torch.tensor as well\ntrain_labels = torch.tensor(df_train_gby['label'])\nvalid_labels = torch.tensor(df_valid_gby['label'])","metadata":{"id":"oi2rx6jyuebH","execution":{"iopub.status.busy":"2022-06-05T12:12:40.607891Z","iopub.execute_input":"2022-06-05T12:12:40.608252Z","iopub.status.idle":"2022-06-05T12:12:40.689235Z","shell.execute_reply.started":"2022-06-05T12:12:40.608219Z","shell.execute_reply":"2022-06-05T12:12:40.688375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 建立Dataloader","metadata":{"id":"J7MkfWIkuebG"}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler","metadata":{"id":"7AAgyFj6uebH","execution":{"iopub.status.busy":"2022-06-05T12:12:40.692196Z","iopub.execute_input":"2022-06-05T12:12:40.692566Z","iopub.status.idle":"2022-06-05T12:12:40.69658Z","shell.execute_reply.started":"2022-06-05T12:12:40.692533Z","shell.execute_reply":"2022-06-05T12:12:40.695765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create dataset and dataloader\n\n# train\ntrain_dataset = TensorDataset(train_tokens, train_types, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_dataset) # randomly sample\ntrain_dataloader = DataLoader(train_dataset, sampler = train_sampler, batch_size = batch_size)","metadata":{"id":"QtPri15wuebH","execution":{"iopub.status.busy":"2022-06-05T12:12:40.697756Z","iopub.execute_input":"2022-06-05T12:12:40.698731Z","iopub.status.idle":"2022-06-05T12:12:40.704316Z","shell.execute_reply.started":"2022-06-05T12:12:40.698693Z","shell.execute_reply":"2022-06-05T12:12:40.703511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# validation\nvalid_dataset = TensorDataset(valid_tokens, valid_types, valid_masks, valid_labels)\nvalid_sampler = SequentialSampler(valid_dataset) # no need to disorganize the order in validation process\nvalid_dataloader = DataLoader(valid_dataset, sampler = valid_sampler, batch_size = batch_size)","metadata":{"id":"qaN4SbEpuebH","execution":{"iopub.status.busy":"2022-06-05T12:12:40.705418Z","iopub.execute_input":"2022-06-05T12:12:40.706429Z","iopub.status.idle":"2022-06-05T12:12:40.71293Z","shell.execute_reply.started":"2022-06-05T12:12:40.706389Z","shell.execute_reply":"2022-06-05T12:12:40.711712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test\ntest_dataset = TensorDataset(test_tokens, test_types, test_masks)\ntest_sampler = SequentialSampler(test_dataset) # no need to disorganize the order in testing process\ntest_dataloader = DataLoader(test_dataset, sampler = test_sampler, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T12:12:40.71454Z","iopub.execute_input":"2022-06-05T12:12:40.715213Z","iopub.status.idle":"2022-06-05T12:12:40.7204Z","shell.execute_reply.started":"2022-06-05T12:12:40.715169Z","shell.execute_reply":"2022-06-05T12:12:40.719627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 模型、優化器和學習率調整","metadata":{"id":"aJi5v4J1uebI"}},{"cell_type":"code","source":"# use an improvised verison of Adam, AdamW, which generally yields better training loss and generalization ability.\n# https://towardsdatascience.com/why-adamw-matters-736223f31b5d\n# Also, we use cosine lr scheduler\n\nfrom transformers import XLNetForSequenceClassification # we use XLNet for sequence classification\nfrom transformers import get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW","metadata":{"id":"fmBKLR-_uebJ","execution":{"iopub.status.busy":"2022-06-05T12:12:40.721733Z","iopub.execute_input":"2022-06-05T12:12:40.722395Z","iopub.status.idle":"2022-06-05T12:12:40.755368Z","shell.execute_reply.started":"2022-06-05T12:12:40.722344Z","shell.execute_reply":"2022-06-05T12:12:40.754698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize the XLNet model and optimizer and lr_scheduler\ndef initialization():\n    model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels = 32)\n    # move to GPU\n    model.to(device)\n    \n    # optimizer, use AdamW\n    optimizer = AdamW(model.parameters(), lr = lr, eps = 1e-8) # default\n    \n    # lr scheduler\n    steps = len(train_dataloader) * epochs # total training steps\n    # you can set warmup if you want. we don't set it here since we use few epochs.\n    warmup_steps = len(train_dataloader) * warmup_epochs\n    scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps = steps)\n    \n    return model, optimizer, scheduler","metadata":{"id":"kQshYX8ruebJ","execution":{"iopub.status.busy":"2022-06-05T12:12:40.756552Z","iopub.execute_input":"2022-06-05T12:12:40.756897Z","iopub.status.idle":"2022-06-05T12:12:40.762647Z","shell.execute_reply.started":"2022-06-05T12:12:40.756863Z","shell.execute_reply":"2022-06-05T12:12:40.7619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialize the model, optimizer and scheduler\nmodel, optimizer, scheduler = initialization()","metadata":{"id":"29MV2h31uebL","outputId":"1d8e5f5e-20a0-449c-ab27-a71dd5e228a0","execution":{"iopub.status.busy":"2022-06-05T12:12:40.763886Z","iopub.execute_input":"2022-06-05T12:12:40.764632Z","iopub.status.idle":"2022-06-05T12:13:33.42187Z","shell.execute_reply.started":"2022-06-05T12:12:40.764595Z","shell.execute_reply":"2022-06-05T12:13:33.421051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 損失函數(分類用XLNet已經包含，所以沒用到了)","metadata":{"id":"4B_hOzELuebJ"}},{"cell_type":"code","source":"# we use crossentropy loss, since multiclass classification\n\n# loss_fn = nn.CrossEntropyLoss()\n# we don't need this when we are using XLNetForSequenceClassification, since it's taken care of this part already.","metadata":{"id":"hUqJ3Nb7uebJ","execution":{"iopub.status.busy":"2022-06-04T13:34:00.549175Z","iopub.execute_input":"2022-06-04T13:34:00.549524Z","iopub.status.idle":"2022-06-04T13:34:00.553975Z","shell.execute_reply.started":"2022-06-04T13:34:00.549485Z","shell.execute_reply":"2022-06-04T13:34:00.552809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 驗證步驟的函數 (訓練函數會用到，所以先定義)","metadata":{"id":"mOuM0dFnuebK"}},{"cell_type":"code","source":"def validate(model, valid_dataloader):\n    # validate in every epoch\n    # set model to evaluation mode\n    model.eval()\n    \n    # tracking variables: the validation F1 scores and losses of every validation batches.\n    valid_F1 = []\n    valid_loss = []\n    \n    # for each batch in validation set, calculate the F1 and loss\n    for batch in valid_dataloader:\n        # move the batch to gpu\n        tokens_batched, types_batched, masks_batched, labels_batched = tuple(PytorchTensor.to(device) for PytorchTensor in batch)\n        \n        # close the gradients and compute predictions\n        with torch.no_grad():\n            # put in token tensors, token types tensor, attention masks tensor and labels tensor, all\n            outputs = model(input_ids = tokens_batched, token_type_ids = types_batched, attention_mask = masks_batched, labels = labels_batched)\n            # the first element of the outputs are the losses\n            loss = outputs[0]\n            # the second element of the outputs are the length-32 tensors, containing the scores of each labels.\n            scores = outputs[1]\n            \n            # get the labels with maximum scores as the predictions.\n            _, predicted_labels = torch.max(outputs[1], dim = 1)\n            \n            # move all the ground truth labels and predictions of this batch to cpu, discard the gradient and turn them into numpy arrays\n            labels_batched = labels_batched.cpu().detach().numpy()\n            predicted_labels = predicted_labels.cpu().detach().numpy()\n            # so that we can call \"f1_score\" to calculate the F1 score of this batch\n            valid_F1.append(f1_score(labels_batched, predicted_labels, average = 'macro') * 100)\n        \n            # compute the loss of this batch. use item() to retrieve the number from the 1-element tensor.\n            valid_loss.append(loss.item())\n    \n    # compute and return the average loss and average F1 score for all batches.\n    return np.mean(valid_loss), np.mean(valid_F1)","metadata":{"id":"95oGldYcuebK","execution":{"iopub.status.busy":"2022-06-05T12:13:33.423314Z","iopub.execute_input":"2022-06-05T12:13:33.423695Z","iopub.status.idle":"2022-06-05T12:13:33.446849Z","shell.execute_reply.started":"2022-06-05T12:13:33.423658Z","shell.execute_reply":"2022-06-05T12:13:33.44604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 測試步驟的函數","metadata":{"id":"K1V8cs4luebL"}},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn","metadata":{"id":"WvWg-9kruebM","execution":{"iopub.status.busy":"2022-06-05T12:13:33.448319Z","iopub.execute_input":"2022-06-05T12:13:33.448741Z","iopub.status.idle":"2022-06-05T12:13:33.456386Z","shell.execute_reply.started":"2022-06-05T12:13:33.448699Z","shell.execute_reply":"2022-06-05T12:13:33.455526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_test(model, test_dataloader):\n    # turn the model into evaluation mode\n    model.eval()\n    \n    # initialize an empty numpy array that stores predictions\n    predictions = np.array([], dtype = np.int64)\n    \n    for batch in test_dataloader:\n        # for each batch in testing data, first move it to the gpu\n        tokens_batched, types_batched, masks_batched = tuple(PytorchTensor.to(device) for PytorchTensor in batch)\n        \n        # close the gradient\n        with torch.no_grad():\n            # pour all token-related tensors into the model and get outputs for this batch.\n            outputs = model(input_ids = tokens_batched, token_type_ids = types_batched, attention_mask = masks_batched)\n            # Use the softmax function to normalize each row (a length-32 tensor) into probabilities that sum up to 1\n            probs = F.softmax(outputs[0], dim = 1)\n            \n            # get the predictions of this batch by picking the labels with the maximum scores.\n            _, predicted_labels = torch.max(probs, dim = 1)\n            \n            # move the predictions tensor to cpu, discard the gradient and turn it into numpy array.\n            predicted_labels = predicted_labels.cpu().detach().numpy()\n        # concatenate predictions of every batch.\n        predictions = np.concatenate((predictions, predicted_labels))\n    \n    return predictions","metadata":{"id":"EB2CPNcSuebM","execution":{"iopub.status.busy":"2022-06-05T12:13:33.457583Z","iopub.execute_input":"2022-06-05T12:13:33.458745Z","iopub.status.idle":"2022-06-05T12:13:33.46866Z","shell.execute_reply.started":"2022-06-05T12:13:33.458716Z","shell.execute_reply":"2022-06-05T12:13:33.467932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 訓練步驟的函數","metadata":{"id":"OwfkCBf7uebK"}},{"cell_type":"code","source":"def train(model, train_dataloader, valid_dataloader = None, validation = True):\n    print(\"Start training...\\n\")\n    \n    for e in range(epochs):\n        '''training'''\n        print(\"Epoch {}:\\n\".format(e+1))\n        print(\"-\" * 70)\n        \n        # Reset tracking variables at the beginning of each epoch: the losses and f1 scores in each batches.\n        total_loss = []\n        total_f1 = []\n        \n        # set the model to training mode\n        model.train()\n        \n        # initialize a progress bar by tqdm module.\n        progress = tqdm(total = len(train_dataloader))\n        # for each batch of trainin data, we have to\n        for step, batch in enumerate(train_dataloader):\n            # load the batch to GPU\n            tokens_batched, types_batched, masks_batched, labels_batched = tuple(PytorchTensor.to(device) for PytorchTensor in batch)\n            \n            # clear the gradients from previous epoch\n            optimizer.zero_grad()\n            \n            # forward propagation, returning predictions\n            # get all tensors (including labels tensor) into the model, and get the outputs\n            outputs = model(input_ids = tokens_batched, token_type_ids = types_batched, attention_mask = masks_batched, labels = labels_batched)\n            # the first element of the outputs is the losses of the batches while the second one is the scores tensor (length-32) of the batches.\n            loss = outputs[0]\n            scores = outputs[1]\n            \n            # get the labels with the maximum scores as the predictions.\n            _, predicted_labels = torch.max(outputs[1], dim = 1)\n            \n            # move the labels tensor and predictions tensor to the cpu, discard the gradient and turn them into numpy arrays.\n            labels_batched = labels_batched.cpu().detach().numpy()\n            predicted_labels = predicted_labels.cpu().detach().numpy()\n            # calculate the F1 scores and add them in the list to store.\n            total_f1.append(f1_score(labels_batched, predicted_labels, average = 'macro') * 100)\n            \n            # add the losses of the batch to the list to store.\n            total_loss.append(loss.item())\n            \n            # backward propagation\n            loss.backward()\n            \n            # clip the gradients s.t. its norm = 1.0, to prevent gradient problem (vanishment, explosion)\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            # update parameters by optimizer and lr by scheduler\n            optimizer.step()\n            scheduler.step()\n            \n            # progress bar update\n            progress.update(1)\n        # after each epoch, calculate the average loss and the average F1 socres over the entire training data\n        average_loss = np.mean(total_loss)\n        average_f1 = np.mean(total_f1)\n        # close the progress bar of the epoch.\n        progress.close()\n        \n        # save model every few epochs if specified.\n        if save_model:\n            if (e+1) % save_model == 0:\n                torch.save(model.state_dict(), \"./NLP_model_params_epoch{}.pth\".format(e+1))\n        \n        '''validation'''\n        if validation:\n            # after each epoch, measure model's val loss and F1 score.\n            val_loss, val_f1 = validate(model, valid_dataloader)\n            \n            print(\"-\" * 70)\n            print(f\"{'Train Loss':^12} | {'Train F1':^10} | {'Val Loss':^10} | {'Val F1':^8}\")\n            # print(f\"{'-':^7} | {average_loss:^12.6f} | {val_loss:^10.6f} | {val_acc:^9.2f} | {elapsed_time:^9.2f}\")\n            print(f\"{average_loss:^12.6f} | {average_f1:^10.2f} | {val_loss:^10.6f} | {val_f1:^8.2f}\")\n            print(\"-\" * 70)\n        print(\"\\n\")\n        \n    print(\"Training Complete!\")","metadata":{"id":"A3p1r6V4uebK","execution":{"iopub.status.busy":"2022-06-05T12:13:33.469725Z","iopub.execute_input":"2022-06-05T12:13:33.472312Z","iopub.status.idle":"2022-06-05T12:13:33.488644Z","shell.execute_reply.started":"2022-06-05T12:13:33.472282Z","shell.execute_reply":"2022-06-05T12:13:33.487891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train the model\ntrain(model, train_dataloader, valid_dataloader)","metadata":{"id":"YLx3FH76uebL","outputId":"38aa5919-5480-4c58-e59e-54a302b4e5e8","execution":{"iopub.status.busy":"2022-06-05T12:13:37.716946Z","iopub.execute_input":"2022-06-05T12:13:37.717528Z","iopub.status.idle":"2022-06-05T13:35:05.473955Z","shell.execute_reply.started":"2022-06-05T12:13:37.717488Z","shell.execute_reply":"2022-06-05T13:35:05.473069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 儲存模型","metadata":{}},{"cell_type":"code","source":"# torch.save(model.state_dict(), \"NLP_model_params.pth\")\n\n'''if you want to load the model parameters you stored, use this! But make sure that your model structure and hyper-parameters settings look exactly the same.'''\n# model, _, _ = initialization()\n# model.load_state_dict(torch.load('../input/parameter/1_63best_400_20_4.pth'))","metadata":{"id":"gGbQn2RAuebM","execution":{"iopub.status.busy":"2022-06-05T12:13:33.490294Z","iopub.execute_input":"2022-06-05T12:13:33.491028Z","iopub.status.idle":"2022-06-05T12:13:37.715507Z","shell.execute_reply.started":"2022-06-05T12:13:33.490968Z","shell.execute_reply":"2022-06-05T12:13:37.714799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 預測測試資料、輸出結果","metadata":{}},{"cell_type":"code","source":"# get the prediction results for the testing data\npredicted_results = predict_test(model, test_dataloader)","metadata":{"id":"YeeSlS8puebM","execution":{"iopub.status.busy":"2022-06-05T13:35:05.475696Z","iopub.execute_input":"2022-06-05T13:35:05.478248Z","iopub.status.idle":"2022-06-05T13:36:37.760579Z","shell.execute_reply.started":"2022-06-05T13:35:05.478217Z","shell.execute_reply":"2022-06-05T13:36:37.759692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_results","metadata":{"id":"03DpD8MzuebN","execution":{"iopub.status.busy":"2022-06-05T13:41:25.564793Z","iopub.execute_input":"2022-06-05T13:41:25.565459Z","iopub.status.idle":"2022-06-05T13:41:25.571482Z","shell.execute_reply.started":"2022-06-05T13:41:25.565422Z","shell.execute_reply":"2022-06-05T13:41:25.570428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make the predictions into a dataframe, with 'conv_id' column of original dataframe as the index.\nresult_df = pd.DataFrame(predicted_results, columns = ['pred'], index = df_test_gby['utterance_cleaned'].index)","metadata":{"id":"FkZxsdlHuebN","execution":{"iopub.status.busy":"2022-06-05T13:36:37.766209Z","iopub.execute_input":"2022-06-05T13:36:37.768574Z","iopub.status.idle":"2022-06-05T13:36:37.775911Z","shell.execute_reply.started":"2022-06-05T13:36:37.768534Z","shell.execute_reply":"2022-06-05T13:36:37.774911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.head()","metadata":{"id":"bmDruUaHuebN","execution":{"iopub.status.busy":"2022-06-05T13:41:21.38056Z","iopub.execute_input":"2022-06-05T13:41:21.380907Z","iopub.status.idle":"2022-06-05T13:41:21.38916Z","shell.execute_reply.started":"2022-06-05T13:41:21.380877Z","shell.execute_reply":"2022-06-05T13:41:21.388311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# merge the predictions to the original testing dataframe, based on the key \"conv_id\" column.\noutput_df = pd.merge(df_test, result_df, on = 'conv_id')","metadata":{"id":"5Mpn83SWuebN","execution":{"iopub.status.busy":"2022-06-05T13:36:37.780438Z","iopub.execute_input":"2022-06-05T13:36:37.784628Z","iopub.status.idle":"2022-06-05T13:36:37.82388Z","shell.execute_reply.started":"2022-06-05T13:36:37.784585Z","shell.execute_reply":"2022-06-05T13:36:37.822948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop all columns except the predictions.\noutput_df.drop(['conv_id', 'utterance_cleaned', 'prompt_cleaned'], axis = 1, inplace = True)","metadata":{"id":"OoDZYeiTuebN","execution":{"iopub.status.busy":"2022-06-05T13:36:37.825443Z","iopub.execute_input":"2022-06-05T13:36:37.826219Z","iopub.status.idle":"2022-06-05T13:36:37.835695Z","shell.execute_reply.started":"2022-06-05T13:36:37.826181Z","shell.execute_reply":"2022-06-05T13:36:37.835025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-05T13:41:18.744472Z","iopub.execute_input":"2022-06-05T13:41:18.745036Z","iopub.status.idle":"2022-06-05T13:41:18.758939Z","shell.execute_reply.started":"2022-06-05T13:41:18.744976Z","shell.execute_reply":"2022-06-05T13:41:18.757894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# output to a csv\noutput_df.to_csv(\"./predicted_results_xlnet.csv\")","metadata":{"id":"bw48SdOZuebN","execution":{"iopub.status.busy":"2022-06-05T13:36:37.837048Z","iopub.execute_input":"2022-06-05T13:36:37.837587Z","iopub.status.idle":"2022-06-05T13:36:37.90863Z","shell.execute_reply.started":"2022-06-05T13:36:37.837551Z","shell.execute_reply":"2022-06-05T13:36:37.907788Z"},"trusted":true},"execution_count":null,"outputs":[]}]}